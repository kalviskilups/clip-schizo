{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    a = torch.zeros([10, 10, 10], dtype = torch.float32, device = 'cuda:3', requires_grad = False)\n",
    "    b = a * a\n",
    "    print(b[0, 0, 0])\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdefdevice\u001b[39;00m\n\u001b[1;32m      2\u001b[0m defdevice\u001b[38;5;241m.\u001b[39mforce_device(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\n",
      "File \u001b[0;32m/tf/FreshCLIPSchitzo/defdevice.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     def_device \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     20\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(def_device)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mforce_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdef_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tf/FreshCLIPSchitzo/defdevice.py:20\u001b[0m, in \u001b[0;36mforce_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m def_device\n\u001b[1;32m     18\u001b[0m def_device \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdef_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:313\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    311\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import defdevice\n",
    "defdevice.force_device('cuda:3')\n",
    "import metrics\n",
    "import nns\n",
    "import immaskdataset\n",
    "import importlib\n",
    "import imcapdataset\n",
    "import embdataset\n",
    "import embedders\n",
    "import losses\n",
    "import imageops\n",
    "import misc\n",
    "import segmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated necessary RAM for loading - 294912000000 bytes\n",
      "Warning, requested to load 1000000 samples, but dataset contained only 566747\n",
      "Estimated necessary RAM for loading - 4096000000 bytes\n",
      "Warning, requested to load 1000000 samples, but dataset contained only 566747\n"
     ]
    }
   ],
   "source": [
    "image_ds = embdataset.RAMLoadDataset('COCOM2F_ALL_d8', load_amount = 1000000, norm = True)\n",
    "text_ds = embdataset.RAMLoadDataset('COCOCLIP_ALL', load_amount = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nns.Linear(text_ds.embedding_shape, image_ds.embedding_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2383, grad_fn=<MeanBackward0>) tensor(0.2631, grad_fn=<MeanBackward0>)\n",
      "1000 tensor(0.1494, grad_fn=<MeanBackward0>) tensor(0.1140, grad_fn=<MeanBackward0>)\n",
      "2000 tensor(0.1555, grad_fn=<MeanBackward0>) tensor(0.1576, grad_fn=<MeanBackward0>)\n",
      "3000 tensor(0.0878, grad_fn=<MeanBackward0>) tensor(0.1581, grad_fn=<MeanBackward0>)\n",
      "4000 tensor(0.1126, grad_fn=<MeanBackward0>) tensor(0.1514, grad_fn=<MeanBackward0>)\n",
      "5000 tensor(0.0994, grad_fn=<MeanBackward0>) tensor(0.1270, grad_fn=<MeanBackward0>)\n",
      "6000 tensor(0.0919, grad_fn=<MeanBackward0>) tensor(0.1184, grad_fn=<MeanBackward0>)\n",
      "7000 tensor(0.0788, grad_fn=<MeanBackward0>) tensor(0.1317, grad_fn=<MeanBackward0>)\n",
      "8000 tensor(0.1008, grad_fn=<MeanBackward0>) tensor(0.1319, grad_fn=<MeanBackward0>)\n",
      "9000 tensor(0.0969, grad_fn=<MeanBackward0>) tensor(0.1082, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def_device = 'cuda:3'\n",
    "\n",
    "image_ds.reset()\n",
    "text_ds.reset()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "batch_count = 10000\n",
    "batch_size = 128\n",
    "optim = torch.optim.Adam(model.parameters, lr = 0.003)\n",
    "\n",
    "image_noise = 0.07 / 10\n",
    "text_noise = 0.15\n",
    "\n",
    "for batch_idx in range(batch_count):\n",
    "    ibatch = image_ds.get_batch(batch_size).to(def_device)\n",
    "    tbatch = text_ds.get_batch(batch_size).to(def_device)\n",
    "    \n",
    "    ibatch = ibatch + torch.rand_like(ibatch) * image_noise\n",
    "    tbatch = tbatch + torch.rand_like(tbatch) * text_noise\n",
    "    \n",
    "    tbatch = model.forward(tbatch)\n",
    "    \n",
    "    neg_ibatch, neg_tbatch = losses.sample_negatives(ibatch, tbatch, batch_size)\n",
    "    \n",
    "    prods = losses.get_products(ibatch, tbatch)\n",
    "    probs = torch.sigmoid(prods)\n",
    "    \n",
    "    neg_prods = losses.get_products(neg_ibatch, neg_tbatch)\n",
    "    neg_probs = torch.sigmoid(neg_prods)\n",
    "    \n",
    "    loss_positive = losses.max_pressure_loss(probs, 1.0)\n",
    "    loss_negative = losses.max_pressure_loss(neg_probs, 0.0)\n",
    "    \n",
    "    loss = (loss_positive + loss_negative) / 2\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    if batch_idx % 1000 == 0:\n",
    "        print(batch_idx, loss_positive, loss_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 31\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdefdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdef_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m train_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_embeddings, requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, device \u001b[38;5;241m=\u001b[39m defdevice\u001b[38;5;241m.\u001b[39mdef_device)\n",
      "File \u001b[0;32m/usr/lib/python3.8/warnings.py:403\u001b[0m, in \u001b[0;36mWarningMessage.__init__\u001b[0;34m(self, message, category, filename, lineno, file, line, source)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWarningMessage\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    400\u001b[0m     _WARNING_DETAILS \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    401\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, category, filename, lineno, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    404\u001b[0m                  line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m=\u001b[39m category\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_embr = embedders.M2FImageEmbedder()\n",
    "text_embr = embedders.CLIPTextEmbedder()\n",
    "\n",
    "ds = immaskdataset.RUGDDataset()\n",
    "\n",
    "test_embeddings = []\n",
    "test_masks = []\n",
    "test_count = 1000\n",
    "\n",
    "train_embeddings = []\n",
    "train_masks = []\n",
    "train_count = 1000\n",
    "\n",
    "labels = ds.labels\n",
    "label_emb = text_embr.forward(labels)\n",
    "\n",
    "for i in range(test_count):\n",
    "    image, masks = ds.get_next()\n",
    "    test_embeddings.append(image_embr.forward(image))\n",
    "    test_masks.append(torch.tensor(masks, requires_grad = False, device = defdevice.def_device))\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "for i in range(train_count):\n",
    "    image, masks = ds.get_next()\n",
    "    train_embeddings.append(image_embr.forward(image))\n",
    "    train_masks.append(torch.tensor(masks, requires_grad = False, device = defdevice.def_device))\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "test_embeddings = torch.tensor(test_embeddings, requires_grad = False, device = defdevice.def_device)\n",
    "train_embeddings = torch.tensor(train_embeddings, requires_grad = False, device = defdevice.def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:1\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:2\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:3\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 38334.69 MB\n",
      "Cached Memory: 51608.00 MB\n",
      "Free Memory: -8892.06 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:4\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:5\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:6\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n",
      "Device: cuda:7\n",
      "Total Memory: 81050.62 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n",
      "Free Memory: 81050.62 MB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda_memory():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device = torch.device(f\"cuda:{i}\")\n",
    "        torch.cuda.set_device(device)\n",
    "        \n",
    "        # Get the current memory allocation and cached memory\n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        cached = torch.cuda.memory_reserved()\n",
    "        \n",
    "        # Get the total GPU memory\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        \n",
    "        # Calculate free memory\n",
    "        free_memory = total_memory - (allocated + cached)\n",
    "        \n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Total Memory: {total_memory / 1024 ** 2:.2f} MB\")\n",
    "        print(f\"Allocated Memory: {allocated / 1024 ** 2:.2f} MB\")\n",
    "        print(f\"Cached Memory: {cached / 1024 ** 2:.2f} MB\")\n",
    "        print(f\"Free Memory: {free_memory / 1024 ** 2:.2f} MB\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 96, 96])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-b4414161b90c>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_embeddings = torch.tensor(test_embeddings, requires_grad = False, device = defdevice.def_device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 8.79 GiB (GPU 3; 79.15 GiB total capacity; 37.44 GiB already allocated; 5.58 GiB free; 50.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdefdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdef_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_embeddings, requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, device \u001b[38;5;241m=\u001b[39m defdevice\u001b[38;5;241m.\u001b[39mdef_device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 8.79 GiB (GPU 3; 79.15 GiB total capacity; 37.44 GiB already allocated; 5.58 GiB free; 50.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "test_embeddings = torch.tensor(test_embeddings, requires_grad = False, device = defdevice.def_device)\n",
    "train_embeddings = torch.tensor(train_embeddings, requires_grad = False, device = defdevice.def_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters, lr = 0.003)\n",
    "\n",
    "epoch_count = 0\n",
    "\n",
    "for j in range(epoch_count)\n",
    "    for i in range(train_count)\n",
    "        trans_embs = model.forward(label_emb)\n",
    "\n",
    "        prods = torch.einsum('eyx,ne->nyx', train_embeddings[i], trans_emb)\n",
    "        probs = torch.sigmoid(prods, dim = 0)\n",
    "\n",
    "        batch_size, y, x = masks.shape\n",
    "\n",
    "        probs_interpolated = F.interpolate(probs.unsqueeze(1), size=(y, x), mode='bilinear', align_corners=False)\n",
    "\n",
    "        probs_interpolated = probs_interpolated.squeeze(1)\n",
    "        \n",
    "        loss = ((probs_interpolated - train_masks[i]) ** 2).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
